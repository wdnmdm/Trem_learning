# Task1

## 绪论与深度学习概述

主要写一下深度学习的定义何主要应用

### 定义：

- 深度学习定义：一般是指通过训练多层网络结构对未知数据进行分类或回归
- 深度学习分类：有监督学习方法——深度前馈网络、卷积神经网络、循环神经网络等；无监督学习方法——深度信念网、深度玻尔兹曼机，深度自编码器等。

### 主要应用

**图像处理领域**

- 图像分类(物体识别)：整幅图像的分类或识别
- 物体检测：检测图像中物体的位置进而识别物体
- 图像分割：对图像中的特定物体按边缘进行分割
- 图像回归：预测图像中物体组成部分的坐标

**语言识别领域主要应用**

- 语言模型：根据之前词预测下一个单词。
- 情感分析：分析文本体现的情感(正负向、正负中或多态度类型)。
- 神经机器翻译：基于统计语言模型的多语种互译。
- 神经自动摘要：根据文本自动生成摘要。
- 机器阅读理解：通过阅读文本回答问题、完成选择题或完型填空。
- 自然语言推理：根据一句话(前提)推理出另一句话(结论)。

**综合应用**

- 图像描述：根据图像给出图像的描述句子
- 可视问答：根据图像或视频回答问题
- 图像生成：根据文本描述生成图像
- 视频生成：根据故事自动生成视频

## 数学基础

数学基础

1、张量、矩阵运算、矩阵的基础知识、矩阵分解  
2、概率统计、常见的（多变量）分布  
3、信息论、熵、互信息、相对熵、交叉熵  
4、最优化估计方法、最小二乘、线性模型

### 矩阵论

#### 矩阵基本知识

**矩阵**:矩阵是一个二维数组，m行n列的实数矩阵，记做$A \in R_{m \times n}$.  
**张量**:张量是一个多维数组，一个$N$阶张量是$N$个向量空间元素的张量积，每个向量空间都有自己的坐标系。张量的阶数
(The order of a tensor)、也称维数(dimension)、模态(modes)、或方式（ways).  
**矩阵的秩(Rank)**：矩阵列向量中的极大线性无关组的数目，记作矩阵的列秩，同样可以定义行秩。行秩=列秩=矩阵的秩，通常记作rank(A)。

**矩阵的逆**

- 若矩阵A为方阵，当 $rank(A_{n×n})<n$时，称A为奇异矩阵或不可逆矩阵；  
- 若矩阵A为方阵，当 $rank(A_{n×n})=n$时，称A为非奇异矩阵或可逆矩阵
其逆矩阵 $A^{-1}$ 满足以下条件，则称 $A^{-1}$ 为矩阵A的逆矩阵： $$ AA^{-1} = A^{-1}A = I_n $$ 其中 $I_n$ 是 $n×n$ 的单位阵。

**矩阵的广义逆矩阵**

如果矩阵不为方阵或者是奇异矩阵，不存在逆矩阵，但是可以计算其广义逆矩阵或者伪逆矩阵；
对于矩阵A，如果存在矩阵 $B$ 使得 $ABA=A$，则称 $B$ 为 $A$ 的广义逆矩阵。

**矩阵分解**
机器学习中常见的矩阵分解有特征分解和奇异值分解。

**特征值和特征向量的定义**

特征值与特征向量的英文是 eigenvalue 和 eigenvector， 这个前缀 eigen- 起源于德语，意思是 proper(这里应该是专属的意思）、characteristic（特征的），其实翻译成’特征‘是很好的翻法。


若矩阵 $A$ 为方阵，则存在非零向量 $x$ 和常数 $\lambda$ 满足 $Ax=\lambda x$，则称 $\lambda$ 为矩阵 $A$ 的一个特征值，$x$ 为矩阵 $A$ 关于 $\lambda$ 的特征向量。
$A_{n \times n}$ 的矩阵具有 $n$ 个特征值，$λ_1 ≤ λ_2 ≤ ⋯ ≤ λ_n$ 其对应的n个特征向量为 $𝒖_1，𝒖_2， ⋯ ，𝒖_𝑛$
矩阵的迹(trace)和行列式(determinant)的值分别为：
$$ \operatorname{tr}(\mathrm{A})=\sum_{i=1}^{n} \lambda_{i} \quad|\mathrm{~A}|=\prod_{i=1}^{n} \lambda_{i} $$


**矩阵特征分解**：$A_{n \times n}$ 的矩阵具有 $n$ 个不同的特征值，那么矩阵A可以分解为 $A = U\Sigma U^{T}$.  
其中 $\Sigma=\left[\begin{array}{cccc}\lambda_{1} & 0 & \cdots & 0 \ 0 & \lambda_{2} & \cdots & 0 \ 0 & 0 & \ddots & \vdots \ 0 & 0 & \cdots & \lambda_{n}\end{array}\right] \quad \mathrm{U}=\left[\boldsymbol{u}{1}, \boldsymbol{u}{2}, \cdots, \boldsymbol{u}{n}\right] \quad \left|\boldsymbol{u}{i}\right|_{2}=1$ .

**奇异值分解**：对于任意矩阵 $A_{m \times n}$，存在正交矩阵 $U_{m \times m}$ 和 $V_{n \times n}$，使其满足 $A = U \Sigma V^{T} \quad U^T U = V^T V = I$，则称上式为矩阵 $A$ 的特征分解。

